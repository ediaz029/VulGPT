#!/usr/bin/env python3
"""Run the Ollama-based vulnerability scanner across checked-out repositories.

This script consumes the checkout manifest produced by ``checkout_repos.py``. For each
package/version pair whose worktree is available, it walks the repository, selects
code files, chunks them, and invokes the existing ``VulnerabilityScanner`` helper to
query the Ollama model. Results are persisted to a JSON artifact suitable for
subsequent review or scoring.
"""
from __future__ import annotations

import argparse
import asyncio
import json
import logging
import os
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence

LOG = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from backend.api.llm.vulnerability_scanner import VulnerabilityScanner, LeadList  # type: ignore  # noqa: E402
from backend.tasks.scoring import ScoringConfig, score_scan_results  # type: ignore  # noqa: E402

DEFAULT_EXTENSIONS = [".py", ".js", ".ts", ".tsx", ".jsx", ".java", ".go", ".php", ".rb", ".cpp", ".c"]
DEFAULT_EXCLUDE_DIRS = {".git", "node_modules", "dist", "build", "__pycache__", "venv", "env"}
BLOCKLIST_EXTENSIONS = {".css", ".lock", ".md", ".min.js", ".scss", ".txt", ".rst", ".svg", ".png", ".jpg", ".gif", ".woff", ".ico"}
HIDDEN_PREFIX = "."
DEFAULT_MAX_FILE_BYTES = 200_000
DEFAULT_INITIAL_CHUNK_SIZE = 200_000
MIN_DYNAMIC_CHUNK_SIZE = 20_000


@dataclass
class ChunkAnalysis:
    file_path: str
    chunk_index: int
    code: str
    leads: List[dict] = field(default_factory=list)
    status: str = "pending"
    error: Optional[str] = None


@dataclass
class VersionScanResult:
    package: str
    version: str
    repo_path: str
    worktree_path: str
    status: str
    chunks: List[ChunkAnalysis] = field(default_factory=list)
    notes: List[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "package": self.package,
            "version": self.version,
            "repo_path": self.repo_path,
            "worktree_path": self.worktree_path,
            "status": self.status,
            "notes": self.notes,
            "chunks": [
                {
                    "file_path": chunk.file_path,
                    "chunk_index": chunk.chunk_index,
                    "status": chunk.status,
                    "error": chunk.error,
                    "leads": chunk.leads,
                }
                for chunk in self.chunks
            ],
        }


@dataclass
class ScanOptions:
    include_ext: Sequence[str]
    exclude_dirs: Sequence[str]
    max_file_bytes: int
    max_files: Optional[int]
    max_chunks: Optional[int]
    chunk_size: int
    adaptive_chunking: bool
    concurrency: int
    dry_run: bool


def build_argparser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Run the Ollama vulnerability scanner across checked-out repos")
    parser.add_argument("--manifest", type=Path, required=True, help="Path to checkout manifest JSON")
    parser.add_argument("--output", type=Path, required=True, help="Destination JSON for scanner findings")
    parser.add_argument("--ollama-url", default="http://localhost:11434", help="Ollama server URL")
    parser.add_argument("--model", default="llama3.2:1b", help="Model name to request from Ollama")
    parser.add_argument("--include-ext", nargs="*", default=DEFAULT_EXTENSIONS, help="File extensions to include (e.g. .py .js)")
    parser.add_argument("--exclude-dirs", nargs="*", default=list(DEFAULT_EXCLUDE_DIRS), help="Directory names to skip during traversal")
    parser.add_argument("--max-file-bytes", type=int, default=DEFAULT_MAX_FILE_BYTES, help="Skip files larger than this many bytes")
    parser.add_argument("--max-files", type=int, default=25, help="Maximum files to analyze per version (None for unlimited)")
    parser.add_argument("--max-chunks", type=int, default=100, help="Maximum chunks per version (None for unlimited)")
    parser.add_argument("--chunk-size", type=int, default=DEFAULT_INITIAL_CHUNK_SIZE, help="Approximate character budget per chunk")
    parser.add_argument("--adaptive-chunking", action="store_true", help="Automatically shrink chunk size when the model reports context overflow")
    parser.add_argument("--concurrency", type=int, default=2, help="Maximum number of concurrent LLM requests")
    parser.add_argument("--max-packages", type=int, default=None, help="Limit total packages processed")
    parser.add_argument("--dry-run", action="store_true", help="Skip LLM calls and emit chunk metadata only")
    parser.add_argument("--score-leads", action="store_true", help="Score detected leads against ground truth via ADK agent")
    parser.add_argument(
        "--scoring-endpoint",
        default=os.getenv("SCORING_ENDPOINT"),
        help="URL of the external scoring service (defaults to SCORING_ENDPOINT env)",
    )
    parser.add_argument(
        "--scoring-api-key",
        default=os.getenv("SCORING_API_KEY"),
        help="Optional API key for the scoring service (defaults to SCORING_API_KEY env)",
    )
    parser.add_argument("--scoring-timeout", type=float, default=60.0, help="Timeout for scoring and OSV lookups in seconds")
    parser.add_argument(
        "--scoring-max-osv-retries",
        type=int,
        default=3,
        help="Number of retries when contacting OSV for ground truth",
    )
    parser.add_argument(
        "--no-local-scoring-fallback",
        action="store_true",
        help="Disable fallback to the local Ollama scorer if the external service fails",
    )
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"], help="Logging level")
    return parser


def load_manifest(manifest_path: Path) -> Dict[str, dict]:
    try:
        return json.loads(manifest_path.read_text())
    except FileNotFoundError as exc:
        raise SystemExit(f"Checkout manifest not found: {manifest_path}") from exc


def build_manifest_index(manifest: dict) -> Dict[str, dict]:
    index: Dict[str, dict] = {}
    for pkg in manifest.get("packages", []):
        name = pkg.get("package")
        if not name:
            continue
        index[name] = pkg
    return index


def iter_scan_targets(manifest: dict, max_packages: Optional[int] = None) -> Iterable[dict]:
    count = 0
    for pkg_entry in manifest.get("packages", []):
        if max_packages and count >= max_packages:
            break
        for version in pkg_entry.get("versions", []):
            if version.get("status") not in {"checked_out", "resolved"}:
                continue
            worktree = version.get("path")
            if not worktree:
                LOG.warning("Version %s of %s lacks a worktree path; skipping", version.get("version"), pkg_entry.get("package"))
                continue
            yield {
                "package": pkg_entry.get("package"),
                "version": version.get("version"),
                "repo_path": pkg_entry.get("repo_path"),
                "worktree_path": worktree,
            }
        count += 1


def should_skip_path(path: Path, exclude_dirs: Sequence[str]) -> bool:
    return any(part in exclude_dirs for part in path.parts)


def collect_code_files(
    worktree: Path,
    include_ext: Sequence[str],
    exclude_dirs: Sequence[str],
    max_files: Optional[int],
    max_file_bytes: int,
) -> List[Path]:
    include_ext_lower = {ext.lower() for ext in include_ext}
    files: List[Path] = []
    for file_path in worktree.rglob("*"):
        if not file_path.is_file():
            continue
        rel_path = file_path.relative_to(worktree)
        if should_skip_path(rel_path, exclude_dirs):
            continue
        if any(part.startswith(HIDDEN_PREFIX) for part in rel_path.parts):
            continue
        suffix = file_path.suffix.lower()
        if suffix in BLOCKLIST_EXTENSIONS:
            continue
        if file_path.stat().st_size > max_file_bytes:
            continue
        if suffix in include_ext_lower:
            files.append(file_path)
    files.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)
    if max_files:
        files = files[:max_files]
    return files


def chunk_content(content: str, chunk_size: int) -> List[str]:
    if len(content) <= chunk_size:
        return [content]
    chunks: List[str] = []
    start = 0
    while start < len(content):
        end = start + chunk_size
        chunk = content[start:end]
        chunks.append(chunk)
        start = end
    return chunks


async def analyze_chunk(
    scanner: VulnerabilityScanner,
    code: str,
    file_label: str,
    semaphore: asyncio.Semaphore,
    dry_run: bool,
) -> Optional[LeadList]:
    if dry_run:
        return None
    async with semaphore:
        return await scanner.analyze_code_chunk(code_chunk=code, file_path=file_label)


async def scan_version(
    scanner: VulnerabilityScanner,
    package: str,
    version: str,
    repo_path: str,
    worktree_path: str,
    options: ScanOptions,
) -> VersionScanResult:
    worktree = Path(worktree_path)
    if not worktree.exists():
        return VersionScanResult(
            package=package,
            version=version,
            repo_path=repo_path,
            worktree_path=worktree_path,
            status="missing_worktree",
            notes=["Worktree path does not exist"],
        )

    files = collect_code_files(
        worktree,
        options.include_ext,
        options.exclude_dirs,
        options.max_files,
        options.max_file_bytes,
    )
    if not files:
        return VersionScanResult(
            package=package,
            version=version,
            repo_path=repo_path,
            worktree_path=worktree_path,
            status="no_files",
            notes=["No code files matched include_ext filters"],
        )

    semaphore = asyncio.Semaphore(max(1, options.concurrency))
    result = VersionScanResult(
        package=package,
        version=version,
        repo_path=repo_path,
        worktree_path=worktree_path,
        status="scanned" if not options.dry_run else "planned",
    )

    remaining_chunks = options.max_chunks
    for file_path in files:
        if remaining_chunks is not None and remaining_chunks <= 0:
            result.notes.append("Max chunk limit reached; remaining code skipped")
            break
        rel_path = file_path.relative_to(worktree)
        file_chunks, remaining_chunks, note = await scan_file_chunks(
            scanner=scanner,
            file_path=file_path,
            rel_path=rel_path,
            semaphore=semaphore,
            options=options,
            remaining_chunks=remaining_chunks,
        )
        result.chunks.extend(file_chunks)
        if note:
            result.notes.append(note)
            break
    return result


async def scan_file_chunks(
    scanner: VulnerabilityScanner,
    file_path: Path,
    rel_path: Path,
    semaphore: asyncio.Semaphore,
    options: ScanOptions,
    remaining_chunks: Optional[int],
) -> tuple[List[ChunkAnalysis], Optional[int], Optional[str]]:
    try:
        content = file_path.read_text(encoding="utf-8", errors="ignore")
    except Exception as exc:  # pragma: no cover
        chunk = ChunkAnalysis(
            file_path=str(rel_path),
            chunk_index=0,
            status="read_error",
            error=str(exc),
            code="",
        )
        return [chunk], remaining_chunks, None

    all_chunks = chunk_content(content, options.chunk_size)
    note: Optional[str] = None
    if remaining_chunks is not None:
        allowed = max(0, remaining_chunks)
        truncated = all_chunks[:allowed]
        if len(truncated) < len(all_chunks):
            note = "Max chunk limit reached; remaining code skipped"
        selected_chunks = truncated
        new_remaining = remaining_chunks - len(truncated)
    else:
        selected_chunks = all_chunks
        new_remaining = remaining_chunks

    chunks: List[ChunkAnalysis] = []
    for idx, chunk_text in enumerate(selected_chunks):
        chunk = await analyze_single_chunk(
            scanner=scanner,
            code=chunk_text,
            rel_path=str(rel_path),
            index=idx,
            semaphore=semaphore,
            dry_run=options.dry_run,
        )
        chunks.append(chunk)

    return chunks, new_remaining, note


async def analyze_single_chunk(
    scanner: VulnerabilityScanner,
    code: str,
    rel_path: str,
    index: int,
    semaphore: asyncio.Semaphore,
    dry_run: bool,
) -> ChunkAnalysis:
    chunk = ChunkAnalysis(
        file_path=rel_path,
        chunk_index=index,
        code=code,
        status="scanning" if not dry_run else "skipped",
    )

    try:
        lead_list = await analyze_chunk(
            scanner=scanner,
            code=code,
            file_label=rel_path,
            semaphore=semaphore,
            dry_run=dry_run,
        )
        if lead_list:
            chunk.leads = [lead.model_dump() for lead in lead_list.leads]
            chunk.status = "complete"
        else:
            chunk.status = "complete" if dry_run else "no_findings"
    except Exception as exc:  # pragma: no cover
        chunk.status = "error"
        chunk.error = str(exc)
    return chunk


async def run_scan(args: argparse.Namespace) -> dict:
    manifest_path = args.manifest.expanduser().resolve()
    output_path = args.output.expanduser().resolve()

    manifest = load_manifest(manifest_path)
    manifest_index = build_manifest_index(manifest)
    options = ScanOptions(
        include_ext=args.include_ext,
        exclude_dirs=args.exclude_dirs,
        max_file_bytes=args.max_file_bytes,
        max_files=args.max_files,
        max_chunks=args.max_chunks,
        chunk_size=args.chunk_size,
        adaptive_chunking=args.adaptive_chunking,
        concurrency=args.concurrency,
        dry_run=args.dry_run,
    )

    scanner = VulnerabilityScanner(ollama_url=args.ollama_url, model_name=args.model)
    results: List[VersionScanResult] = []
    results_payload: List[Dict[str, Any]] = []
    scored_packages: List[Dict[str, Any]] = []
    scoring_metrics: Dict[str, Any] = {}

    try:
        for target in iter_scan_targets(manifest, args.max_packages):
            LOG.info("Scanning %s@%s", target["package"], target["version"])
            version_result = await scan_version(
                scanner=scanner,
                package=target["package"],
                version=target["version"],
                repo_path=target["repo_path"],
                worktree_path=target["worktree_path"],
                options=options,
            )
            results.append(version_result)

        results_payload = [res.to_dict() for res in results]
        if args.score_leads:
            if args.dry_run:
                LOG.warning("Scoring skipped because --dry-run is enabled")
            else:
                scoring_config = ScoringConfig(
                    enabled=True,
                    endpoint=args.scoring_endpoint,
                    api_key=args.scoring_api_key,
                    timeout=args.scoring_timeout,
                    use_local_fallback=not args.no_local_scoring_fallback,
                    max_osv_retries=args.scoring_max_osv_retries,
                )
                scored_packages, scoring_metrics = await score_scan_results(
                    scan_payload={"results": results_payload},
                    manifest_index=manifest_index,
                    scoring_config=scoring_config,
                    scanner=scanner,
                )
                totals = scoring_metrics.get("totals", {})
                if totals:
                    LOG.info(
                        "Scoring totals: TP=%s FP=%s FN=%s precision=%s recall=%s",
                        totals.get("true_positives"),
                        totals.get("false_positives"),
                        totals.get("false_negatives"),
                        totals.get("precision"),
                        totals.get("recall"),
                    )
    finally:
        await scanner.close()

    payload = {
        "options": {
            "ollama_url": args.ollama_url,
            "model": args.model,
            "include_ext": list(options.include_ext),
            "exclude_dirs": list(options.exclude_dirs),
            "max_file_bytes": options.max_file_bytes,
            "max_files": options.max_files,
            "max_chunks": options.max_chunks,
            "chunk_size": options.chunk_size,
            "adaptive_chunking": options.adaptive_chunking,
            "concurrency": options.concurrency,
            "dry_run": options.dry_run,
        },
        "results": results_payload or [res.to_dict() for res in results],
        "stats": summarize_scan(results),
    }
    if scored_packages:
        payload["scoring"] = scored_packages
    if scoring_metrics:
        payload["metrics"] = scoring_metrics

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, indent=2))
    LOG.info("Wrote scanner output to %s", output_path)
    LOG.info("Scan status counts: %s", payload["stats"])

    return payload


def summarize_scan(results: Sequence[VersionScanResult]) -> Dict[str, int]:
    counts: Dict[str, int] = {}
    for res in results:
        counts[res.status] = counts.get(res.status, 0) + 1
        for chunk in res.chunks:
            key = f"chunk_{chunk.status}"
            counts[key] = counts.get(key, 0) + 1
    return counts


def main(argv: Optional[List[str]] = None) -> int:
    parser = build_argparser()
    args = parser.parse_args(argv)

    logging.basicConfig(level=getattr(logging, args.log_level.upper()), format="%(levelname)s:%(name)s:%(message)s")

    try:
        asyncio.run(run_scan(args))
    except KeyboardInterrupt:  # pragma: no cover
        LOG.warning("Scan interrupted by user")
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
