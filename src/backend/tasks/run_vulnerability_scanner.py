#!/usr/bin/env python3
"""Run the Ollama-based vulnerability scanner across checked-out repositories.

This script consumes the checkout manifest produced by ``checkout_repos.py``. For each
package/version pair whose worktree is available, it walks the repository, selects
code files, chunks them, and invokes the existing ``VulnerabilityScanner`` helper to
query the Ollama model. Results are persisted to a JSON artifact suitable for
subsequent review or scoring.
"""
from __future__ import annotations

import argparse
import asyncio
import json
import logging
import os
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence

LOG = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from backend.api.llm.vulnerability_scanner import VulnerabilityScanner, LeadList  # type: ignore  # noqa: E402

DEFAULT_EXTENSIONS = [".py", ".js", ".ts", ".tsx", ".jsx"]
DEFAULT_EXCLUDE_DIRS = {".git", "node_modules", "dist", "build", "__pycache__"}


@dataclass
class ChunkAnalysis:
    file_path: str
    chunk_index: int
    code: str
    leads: List[dict] = field(default_factory=list)
    status: str = "pending"
    error: Optional[str] = None


@dataclass
class VersionScanResult:
    package: str
    version: str
    repo_path: str
    worktree_path: str
    status: str
    chunks: List[ChunkAnalysis] = field(default_factory=list)
    notes: List[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "package": self.package,
            "version": self.version,
            "repo_path": self.repo_path,
            "worktree_path": self.worktree_path,
            "status": self.status,
            "notes": self.notes,
            "chunks": [
                {
                    "file_path": chunk.file_path,
                    "chunk_index": chunk.chunk_index,
                    "status": chunk.status,
                    "error": chunk.error,
                    "leads": chunk.leads,
                }
                for chunk in self.chunks
            ],
        }


@dataclass
class ScanOptions:
    include_ext: Sequence[str]
    exclude_dirs: Sequence[str]
    max_files: Optional[int]
    max_chunks: Optional[int]
    chunk_size: int
    concurrency: int
    dry_run: bool


def build_argparser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Run the Ollama vulnerability scanner across checked-out repos")
    parser.add_argument("--manifest", type=Path, required=True, help="Path to checkout manifest JSON")
    parser.add_argument("--output", type=Path, required=True, help="Destination JSON for scanner findings")
    parser.add_argument("--ollama-url", default="http://localhost:11434", help="Ollama server URL")
    parser.add_argument("--model", default="llama3.2:1b", help="Model name to request from Ollama")
    parser.add_argument("--include-ext", nargs="*", default=DEFAULT_EXTENSIONS, help="File extensions to include (e.g. .py .js)")
    parser.add_argument("--exclude-dirs", nargs="*", default=list(DEFAULT_EXCLUDE_DIRS), help="Directory names to skip during traversal")
    parser.add_argument("--max-files", type=int, default=25, help="Maximum files to analyze per version (None for unlimited)")
    parser.add_argument("--max-chunks", type=int, default=100, help="Maximum chunks per version (None for unlimited)")
    parser.add_argument("--chunk-size", type=int, default=3000, help="Approximate character budget per chunk")
    parser.add_argument("--concurrency", type=int, default=2, help="Maximum number of concurrent LLM requests")
    parser.add_argument("--max-packages", type=int, default=None, help="Limit total packages processed")
    parser.add_argument("--dry-run", action="store_true", help="Skip LLM calls and emit chunk metadata only")
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"], help="Logging level")
    return parser


def load_manifest(manifest_path: Path) -> Dict[str, dict]:
    try:
        return json.loads(manifest_path.read_text())
    except FileNotFoundError as exc:
        raise SystemExit(f"Checkout manifest not found: {manifest_path}") from exc


def iter_scan_targets(manifest: dict, max_packages: Optional[int] = None) -> Iterable[dict]:
    count = 0
    for pkg_entry in manifest.get("packages", []):
        if max_packages and count >= max_packages:
            break
        for version in pkg_entry.get("versions", []):
            if version.get("status") not in {"checked_out", "resolved"}:
                continue
            worktree = version.get("path")
            if not worktree:
                LOG.warning("Version %s of %s lacks a worktree path; skipping", version.get("version"), pkg_entry.get("package"))
                continue
            yield {
                "package": pkg_entry.get("package"),
                "version": version.get("version"),
                "repo_path": pkg_entry.get("repo_path"),
                "worktree_path": worktree,
            }
        count += 1


def should_skip_path(path: Path, exclude_dirs: Sequence[str]) -> bool:
    return any(part in exclude_dirs for part in path.parts)


def collect_code_files(worktree: Path, include_ext: Sequence[str], exclude_dirs: Sequence[str], max_files: Optional[int]) -> List[Path]:
    include_ext_lower = {ext.lower() for ext in include_ext}
    files: List[Path] = []
    for file_path in worktree.rglob("*"):
        if not file_path.is_file():
            continue
        if should_skip_path(file_path.relative_to(worktree), exclude_dirs):
            continue
        if file_path.suffix.lower() in include_ext_lower:
            files.append(file_path)
    files.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)
    if max_files:
        files = files[:max_files]
    return files


def chunk_content(content: str, chunk_size: int) -> List[str]:
    if len(content) <= chunk_size:
        return [content]
    chunks: List[str] = []
    start = 0
    while start < len(content):
        end = start + chunk_size
        chunk = content[start:end]
        chunks.append(chunk)
        start = end
    return chunks


async def analyze_chunk(
    scanner: VulnerabilityScanner,
    code: str,
    file_label: str,
    semaphore: asyncio.Semaphore,
    dry_run: bool,
) -> Optional[LeadList]:
    if dry_run:
        return None
    async with semaphore:
        return await scanner.analyze_code_chunk(code_chunk=code, file_path=file_label)


async def scan_version(
    scanner: VulnerabilityScanner,
    package: str,
    version: str,
    repo_path: str,
    worktree_path: str,
    options: ScanOptions,
) -> VersionScanResult:
    worktree = Path(worktree_path)
    if not worktree.exists():
        return VersionScanResult(
            package=package,
            version=version,
            repo_path=repo_path,
            worktree_path=worktree_path,
            status="missing_worktree",
            notes=["Worktree path does not exist"],
        )

    files = collect_code_files(worktree, options.include_ext, options.exclude_dirs, options.max_files)
    if not files:
        return VersionScanResult(
            package=package,
            version=version,
            repo_path=repo_path,
            worktree_path=worktree_path,
            status="no_files",
            notes=["No code files matched include_ext filters"],
        )

    semaphore = asyncio.Semaphore(max(1, options.concurrency))
    result = VersionScanResult(
        package=package,
        version=version,
        repo_path=repo_path,
        worktree_path=worktree_path,
        status="scanned" if not options.dry_run else "planned",
    )

    remaining_chunks = options.max_chunks
    for file_path in files:
        if remaining_chunks is not None and remaining_chunks <= 0:
            result.notes.append("Max chunk limit reached; remaining code skipped")
            break
        rel_path = file_path.relative_to(worktree)
        file_chunks, remaining_chunks, note = await scan_file_chunks(
            scanner=scanner,
            file_path=file_path,
            rel_path=rel_path,
            semaphore=semaphore,
            options=options,
            remaining_chunks=remaining_chunks,
        )
        result.chunks.extend(file_chunks)
        if note:
            result.notes.append(note)
            break
    return result


async def scan_file_chunks(
    scanner: VulnerabilityScanner,
    file_path: Path,
    rel_path: Path,
    semaphore: asyncio.Semaphore,
    options: ScanOptions,
    remaining_chunks: Optional[int],
) -> tuple[List[ChunkAnalysis], Optional[int], Optional[str]]:
    try:
        content = file_path.read_text(encoding="utf-8", errors="ignore")
    except Exception as exc:  # pragma: no cover
        chunk = ChunkAnalysis(
            file_path=str(rel_path),
            chunk_index=0,
            status="read_error",
            error=str(exc),
            code="",
        )
        return [chunk], remaining_chunks, None

    all_chunks = chunk_content(content, options.chunk_size)
    note: Optional[str] = None
    if remaining_chunks is not None:
        allowed = max(0, remaining_chunks)
        truncated = all_chunks[:allowed]
        if len(truncated) < len(all_chunks):
            note = "Max chunk limit reached; remaining code skipped"
        selected_chunks = truncated
        new_remaining = remaining_chunks - len(truncated)
    else:
        selected_chunks = all_chunks
        new_remaining = remaining_chunks

    chunks: List[ChunkAnalysis] = []
    for idx, chunk_text in enumerate(selected_chunks):
        chunk = await analyze_single_chunk(
            scanner=scanner,
            code=chunk_text,
            rel_path=str(rel_path),
            index=idx,
            semaphore=semaphore,
            dry_run=options.dry_run,
        )
        chunks.append(chunk)

    return chunks, new_remaining, note


async def analyze_single_chunk(
    scanner: VulnerabilityScanner,
    code: str,
    rel_path: str,
    index: int,
    semaphore: asyncio.Semaphore,
    dry_run: bool,
) -> ChunkAnalysis:
    chunk = ChunkAnalysis(
        file_path=rel_path,
        chunk_index=index,
        code=code,
        status="scanning" if not dry_run else "skipped",
    )

    try:
        lead_list = await analyze_chunk(
            scanner=scanner,
            code=code,
            file_label=rel_path,
            semaphore=semaphore,
            dry_run=dry_run,
        )
        if lead_list:
            chunk.leads = [lead.model_dump() for lead in lead_list.leads]
            chunk.status = "complete"
        else:
            chunk.status = "complete" if dry_run else "no_findings"
    except Exception as exc:  # pragma: no cover
        chunk.status = "error"
        chunk.error = str(exc)
    return chunk


async def run_scan(args: argparse.Namespace) -> dict:
    manifest_path = args.manifest.expanduser().resolve()
    output_path = args.output.expanduser().resolve()

    manifest = load_manifest(manifest_path)
    options = ScanOptions(
        include_ext=args.include_ext,
        exclude_dirs=args.exclude_dirs,
        max_files=args.max_files,
        max_chunks=args.max_chunks,
        chunk_size=args.chunk_size,
        concurrency=args.concurrency,
        dry_run=args.dry_run,
    )

    scanner = VulnerabilityScanner(ollama_url=args.ollama_url, model_name=args.model)
    results: List[VersionScanResult] = []

    try:
        for target in iter_scan_targets(manifest, args.max_packages):
            LOG.info("Scanning %s@%s", target["package"], target["version"])
            version_result = await scan_version(
                scanner=scanner,
                package=target["package"],
                version=target["version"],
                repo_path=target["repo_path"],
                worktree_path=target["worktree_path"],
                options=options,
            )
            results.append(version_result)
    finally:
        await scanner.close()

    payload = {
        "options": {
            "ollama_url": args.ollama_url,
            "model": args.model,
            "include_ext": list(options.include_ext),
            "exclude_dirs": list(options.exclude_dirs),
            "max_files": options.max_files,
            "max_chunks": options.max_chunks,
            "chunk_size": options.chunk_size,
            "concurrency": options.concurrency,
            "dry_run": options.dry_run,
        },
        "results": [res.to_dict() for res in results],
        "stats": summarize_scan(results),
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, indent=2))
    LOG.info("Wrote scanner output to %s", output_path)
    LOG.info("Scan status counts: %s", payload["stats"])

    return payload


def summarize_scan(results: Sequence[VersionScanResult]) -> Dict[str, int]:
    counts: Dict[str, int] = {}
    for res in results:
        counts[res.status] = counts.get(res.status, 0) + 1
        for chunk in res.chunks:
            key = f"chunk_{chunk.status}"
            counts[key] = counts.get(key, 0) + 1
    return counts


def main(argv: Optional[List[str]] = None) -> int:
    parser = build_argparser()
    args = parser.parse_args(argv)

    logging.basicConfig(level=getattr(logging, args.log_level.upper()), format="%(levelname)s:%(name)s:%(message)s")

    try:
        asyncio.run(run_scan(args))
    except KeyboardInterrupt:  # pragma: no cover
        LOG.warning("Scan interrupted by user")
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
