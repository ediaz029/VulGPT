from neo4j import GraphDatabase
import json
import time
from datetime import datetime
import logging
from osv.neo4j_connection import get_neo4j_driver
import math
from ortools.sat.python import cp_model
import re

class VulnerabilityRepoMapper:
    def __init__(self, batch_size=5000):
        self._driver = None
        self.batch_size = batch_size  # Number of records to process in each batch
        self.package_cve_versions_last = ""

    def connect(self):
        self._driver = get_neo4j_driver()
        return self._driver is not None

    def close(self):
        """Close the Neo4j connection if open"""
        if self._driver:
            self._driver.close()
            print("Neo4j connection closed.")

    def get_vulnerability_count(self):
        """Get the total count of vulnerabilities in the database for a specific repo"""
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return 0

        with self._driver.session() as session:
            query = """
            MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
            MATCH (v)-[:AFFECTS]->(p:Package)
            WHERE vr.name = 'OSV'
            RETURN COUNT(*) AS count
            """
            result = session.run(query)
            record = result.single()
            return record["count"] if record else 0

    def get_vulnerability_repo_mapping_batched(self, repo_name="OSV", progress_interval=10000):
        """
        Create a nested dictionary mapping vuln repositories to their vulnerabilities and affected versions.

        Output structure:
        {
          "OSV": {
            "CVE-1234": ["v1.0", "v1.2"],
            "CVE-5678": ["v1.1", "v1.3"],
            ...
          }
        }
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return {}

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerabilities for repo '{repo_name}'...")

        vuln_repo_map = {}
        processed_count = 0
        start_time = time.time()

        with self._driver.session() as session:
            skip = 0

            while True:
                query = f"""
                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                MATCH (v)-[:AFFECTS]->(p:Package)
                WHERE vr.name = $repo_name
                RETURN vr.name AS repo_name, v.id AS vuln_id, p.versions AS affected_versions, p.purl AS purl
                SKIP {skip} LIMIT {self.batch_size}
                """

                results = list(session.run(query, {"repo_name": repo_name}))
                if not results:
                    break

                for record in results:
                    repo_name_db = record['repo_name']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']
                    purl = record.get('purl', None)

                    if repo_name_db not in vuln_repo_map:
                        vuln_repo_map[repo_name_db] = {}

                    if vuln_id not in vuln_repo_map[repo_name_db]:
                        vuln_repo_map[repo_name][vuln_id] = {
                            'versions': [],
                            'purl': purl
                        }

                    # If p.versions is a single string, add it directly, else extend the list
                    if isinstance(affected_versions, list):
                        for version in affected_versions:
                            if version not in vuln_repo_map[repo_name][vuln_id]['versions']:
                                vuln_repo_map[repo_name][vuln_id]['versions'].append(version)
                    else:
                        if affected_versions not in vuln_repo_map[repo_name][vuln_id]['versions']:
                            vuln_repo_map[repo_name][vuln_id]['versions'].append(affected_versions)

                batch_size_here = len(results)
                processed_count += batch_size_here

                if processed_count % progress_interval < self.batch_size:
                    elapsed = time.time() - start_time
                    percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                    rps = processed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0

                    print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                          f"Speed: {rps:.1f} records/sec - "
                          f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                skip += batch_size_here
                if batch_size_here < self.batch_size:
                    break

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        total_vulns_found = sum(len(vulns) for vulns in vuln_repo_map.values())
        print(f"Found {total_vulns_found} unique vulnerabilities")

        return vuln_repo_map

    def build_minimal_hitting_sets_per_package(self, input_file=None, output_file=None, repo_name="OSV", batch_size=100):
        """
        Build minimal hitting sets for each package using data from a previously generated JSON file.
        
        Args:
            input_file: Path to JSON file generated by export_to_json or export_to_json_streaming (optional)
                    If None, will use the last exported file stored in self.package_cve_versions_last
            output_file: Output file name for the results (optional)
            repo_name: Repository name for reference (default: "OSV")
            batch_size: Number of packages to process in each batch
            
        Returns:
            Dictionary containing package-organized data with minimal hitting sets
            (only includes ecosystem, minimal_versions, and min_cover_size)
        """
        import time
        import logging
        import json
        
        logger = logging.getLogger(__name__)
        
        # If no input file is specified, use the last exported file
        if not input_file:
            if self.package_cve_versions_last:
                input_file = self.package_cve_versions_last
                print(f"Using last exported file: {input_file}")
            else:
                print("Error: No input file specified and no previous export found. Please specify an input_file or run export_to_json/export_to_json_streaming first.")
                return None
        
        logger.info(f"Starting build_minimal_hitting_sets_per_package using data from {input_file}")
        
        start_time = time.time()
        
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f'package_minimal_sets_{repo_name}_{timestamp}.json'
        
        # Initialize results structure
        results = {}
        
        try:
            # Read the input JSON file
            logger.info(f"Reading vulnerability data from file: {input_file}")
            try:
                with open(input_file, 'r') as f:
                    print(f"Loading data from {input_file}...")
                    data = json.load(f)
                
                # Check which format the data is in
                if repo_name in data:
                    # Format from export_to_json - vulnerability-centric data
                    print(f"Detected export_to_json format (vulnerability-centric)")
                    vuln_data = data[repo_name]
                    print(f"Found {len(vuln_data)} vulnerabilities in {repo_name} repo")
                    
                    # We need to transform to package-centric format
                    # This is a complex process that depends on data structure
                    # For now, we'll assume we need to use the Neo4j approach instead
                    print("Warning: Reading from vulnerability-centric format not yet supported.")
                    print("Please use export_to_json_streaming format instead.")
                    return None
                    
                else:
                    # Assume format from export_to_json_streaming - already package-centric
                    print(f"Detected export_to_json_streaming format (package-centric)")
                    package_data = data
                    all_packages = list(package_data.keys())
                    print(f"Found {len(all_packages)} packages in input file")
            
            except json.JSONDecodeError:
                print(f"Error: {input_file} is not a valid JSON file")
                return None
            except Exception as e:
                print(f"Error reading input file: {e}")
                return None
            
            # Process packages in batches
            processed = 0
            successful = 0
            
            for i in range(0, len(all_packages), batch_size):
                batch = all_packages[i:i+batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(all_packages) - 1) // batch_size + 1
                
                print(f"Processing batch {batch_num}/{total_batches} ({len(batch)} packages)")
                
                for package_name in batch:
                    try:
                        # Get package data from the loaded file
                        if package_name in package_data:
                            package_info = package_data[package_name]
                            
                            # Extract ecosystem if available
                            ecosystem = package_info.get("ecosystem", None)
                            
                            # Extract vulnerability data for processing (not for storing in results)
                            vulns = {}

                            for key, value in package_info.items():
                                if key not in ["ecosystem", "purl", "minimal_versions", "min_cover_size"]:
                                    # This key is a vulnerability ID (e.g., "CVE-2024-1234")
                                    vuln_id = key
        
                                    # Value should be the list of affected versions
                                    if isinstance(value, list):
                                        # Already a list - filter out None/empty strings
                                        version_list = [v for v in value if v]
                                        if version_list:
                                            vulns[vuln_id] = version_list
                                    elif isinstance(value, str) and value:
                                        # Single version string
                                        vulns[vuln_id] = [value]
                                    else:
                                        # Skip invalid/empty values
                                        continue                
                            
                            if not vulns:
                                print(f"  - Package {package_name}: No vulnerabilities found, skipping")
                                continue
                        else:
                            print(f"  - Package {package_name}: Not found in input file, skipping")
                            continue
                        
                        # Extract version lists for this package (list of lists)
                        version_lists = [versions for versions in vulns.values() if versions]  # Filter empty lists

                        if not version_lists:
                            print(f"  - Package {package_name}: No valid version lists, skipping")
                            continue

                        # Generate recency scores for this package
                        version_recency = self._get_semantic_version_recency(version_lists)

                        # Find minimum hitting set for this package
                        start_solve = time.time()
                        minimal_cover = find_minimum_hitting_set(version_lists, version_recency)

                        solve_time = time.time() - start_solve

                        # Store in results
                        purl = package_info.get("purl", "")
                        results[package_name] = {
                            "ecosystem": ecosystem,
                            "purl": purl,
                            "minimal_versions": minimal_cover,
                            "min_cover_size": len(minimal_cover),
                            "vulnerability_count": len(vulns)  # Also track how many CVEs this covers
                        }
                        
                        successful += 1
                        
                        print(f"  - Package {package_name}: {len(vulns)} vulnerabilities, {len(minimal_cover)} versions in minimal set ({solve_time:.1f}s)")
                    
                    except Exception as e:
                        print(f"  - Error processing package {package_name}: {e}")
                    
                    processed += 1
                
                # Save intermediate results after each batch
                try:
                    with open(output_file, 'w') as f:
                        json.dump(results, f, indent=2)
                    
                    print(f"Saved intermediate results ({len(results)} packages) to {output_file}")
                except Exception as e:
                    print(f"Error writing intermediate results: {e}")
                
                # Progress report
                elapsed = time.time() - start_time
                package_count = len(all_packages)
                print(f"Progress: {processed}/{package_count} packages ({processed/package_count*100:.1f}%) - "
                    f"Success rate: {successful}/{processed} ({(successful/processed*100) if processed > 0 else 0:.1f}%) - "
                    f"Elapsed: {elapsed:.1f}s")
            
            # Write final results to file
            try:
                with open(output_file, 'w') as f:
                    json.dump(results, f, indent=2)
                print(f"Exported package-based minimal hitting sets to {output_file}")
            except Exception as e:
                print(f"Error writing to file: {e}")
            
            total_time = time.time() - start_time
            print(f"Completed in {total_time:.1f} seconds")
            return results
            
        except Exception as e:
            print(f"Error building minimal hitting sets per package: {e}")
            import traceback
            traceback.print_exc()
            return results if results else None
                    

    def _store_minimal_cover_in_neo4j(self, repo_name, cover_list):
        """
        Store the minimal version cover in Neo4j for a given repository.
        
        Args:
            repo_name: Name of the repository (e.g., "OSV")
            cover_list: List of versions making up the minimal cover
            
        Returns:
            None
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return
        
        if not cover_list:
            print(f"Warning: Empty minimal cover for {repo_name}, skipping Neo4j update.")
            return
        
        print(f"Storing minimal cover with {len(cover_list)} versions for repo {repo_name} in Neo4j...")
        
        try:
            with self._driver.session() as session:
                # Update or create the VULN_REPO node with the minimal_versions property
                query = """
                MERGE (r:VULN_REPO {name: $repo_name})
                SET r.minimal_versions = $versions,
                    r.minimal_versions_count = size($versions),
                    r.minimal_versions_updated = datetime()
                RETURN r.name as repo_name, r.minimal_versions_count as count
                """
                
                result = session.run(
                    query, 
                    repo_name=repo_name, 
                    versions=cover_list
                )
                
                # Get the first record from the result to confirm success
                record = result.single()
                if record:
                    print(f"Successfully updated {record['repo_name']} with {record['count']} minimal versions in Neo4j.")
                else:
                    print(f"Warning: Query executed but no confirmation returned for {repo_name}.")
        
        except Exception as e:
            print(f"Error storing minimal cover in Neo4j: {e}")


    def _get_semantic_version_recency(self, cve_version_lists):
        """
        Create a recency score mapping for all versions that properly handles
        semantic versioning patterns (e.g., v1.2.3, 1.2.3-alpha, etc.).
        
        Args:
            cve_version_lists: List of lists containing affected versions for each CVE
            
        Returns:
            Dictionary mapping each version to a recency score (higher = more recent)
        """
        import re
        from datetime import datetime
        
        # Collect all unique versions
        all_versions = set()
        for sublist in cve_version_lists:
            all_versions.update(sublist)
        
        # Define regex patterns for version strings
        # Pattern for semantic versions like v1.2.3 or 1.2.3-alpha
        semver_pattern = re.compile(r'^v?(\d+)(?:[.-](\d+))?(?:[.-](\d+))?(?:[.-](\d+))?(?:[.-]([a-zA-Z0-9-]+))?$')
        # Pattern for date-based versions like 2021-03-15 or 20210315
        date_pattern = re.compile(r'^(\d{4})-?(\d{2})-?(\d{2})$')
        
        # Store parsed version components
        version_components = {}
        
        # Process each version string
        for version in all_versions:
            # Try semver pattern first
            semver_match = semver_pattern.match(version)
            if semver_match:
                # Extract numeric components, fill with zeros if missing
                components = []
                for i in range(1, 5):  # Groups 1-4 are version numbers
                    group = semver_match.group(i)
                    components.append(int(group) if group and group.isdigit() else 0)
                
                # Check for suffix (alpha, beta, etc.) that should reduce priority
                suffix = semver_match.group(5)
                suffix_penalty = 0
                if suffix:
                    # Pre-releases should be ranked lower than final releases
                    suffix_lower = suffix.lower()
                    if 'alpha' in suffix_lower:
                        suffix_penalty = -30
                    elif 'beta' in suffix_lower:
                        suffix_penalty = -20
                    elif 'rc' in suffix_lower or 'pre' in suffix_lower:
                        suffix_penalty = -10
                
                # Add suffix penalty to last component
                components[3] += suffix_penalty
                
                version_components[version] = components
                continue
            
            # Try date pattern
            date_match = date_pattern.match(version)
            if date_match:
                try:
                    year, month, day = map(int, date_match.groups())
                    # Convert date to numeric value (days since epoch)
                    date_obj = datetime(year, month, day)
                    epoch = datetime(1970, 1, 1)
                    days = (date_obj - epoch).days
                    # Use high first component to rank dates above regular versions
                    version_components[version] = [10000, days, 0, 0]
                    continue
                except (ValueError, OverflowError):
                    # Invalid date, fall through to default handling
                    pass
            
            # Default handling for non-matched versions
            # Convert to string and use character codes as components
            version_components[version] = [0, 0, 0, 0]  # Lowest priority
        
        # Sort versions based on components
        sorted_versions = sorted(version_components.items(), key=lambda x: tuple(x[1]))
        
        # Create recency scores (higher = more recent)
        version_recency = {}
        base_score = 100
        for i, (version, _) in enumerate(sorted_versions):
            version_recency[version] = base_score + i
        
        return version_recency

    def export_to_json(self, repo_name="OSV", filename=None):
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'vulnerability_repo_map_{repo_name}_{timestamp}.json'
        self.package_cve_versions_last = filename
        print(f"Starting export to {filename}...")
        start_time = time.time()

        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name)
        if not vuln_repo_map:
            print("No data to export.")
            return False

        with open(filename, 'w') as f:
            json.dump(vuln_repo_map, f, indent=2)

        print(f"Export completed in {time.time() - start_time:.1f} seconds")
        print(f"Exported vulnerability repo mapping to {filename}")
        return True


    def export_to_json_streaming(self, filename=None, progress_interval=10000, batch_size=50000):
        """
        Export vulnerability data in batches to avoid Neo4j memory issues.
        Uses SKIP/LIMIT to process in manageable chunks.
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return False

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_cve_versions_{timestamp}.json'
        self.package_cve_versions_last = filename
        
        print(f"Starting batched streaming export to {filename}...")
        start_time = time.time()

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerability relationships in batches of {batch_size}...")

        # Build the data in memory first (package-centric)
        package_data = {}
        processed_count = 0
        offset = 0

        while offset < total_count:
            with self._driver.session() as session:
                # Query with SKIP/LIMIT for batching
                query = """
                    MATCH (v:Vulnerability)-[:AFFECTS]->(p:Package)
                    RETURN p.name AS package_name, 
                        p.ecosystem AS ecosystem, 
                        p.purl AS purl, 
                        v.id AS vuln_id, 
                        p.versions AS affected_versions
                    ORDER BY p.name, v.id
                    SKIP $offset
                    LIMIT $batch_size
                """
                
                result = session.run(query, offset=offset, batch_size=batch_size)
                
                batch_count = 0
                for record in result:
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    purl = record.get('purl', '')
                    vuln_id = record['vuln_id']
                    versions = record['affected_versions']
                    
                    # Initialize package if not seen before
                    if package_name not in package_data:
                        package_data[package_name] = {
                            'ecosystem': ecosystem,
                            'purl': purl
                        }
                    
                    # Normalize versions to list
                    if isinstance(versions, list):
                        version_list = versions
                    else:
                        version_list = [versions] if versions else []
                    
                    # Store unique versions for this vulnerability
                    if vuln_id in package_data[package_name]:
                        # Merge with existing versions
                        existing = set(package_data[package_name][vuln_id])
                        existing.update(version_list)
                        package_data[package_name][vuln_id] = list(existing)
                    else:
                        package_data[package_name][vuln_id] = list(set(version_list))
                    
                    batch_count += 1
                    processed_count += 1
                
                # Progress update
                elapsed = time.time() - start_time
                percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                rps = processed_count / elapsed if elapsed > 0 else 0
                eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0
                
                print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                    f"Packages: {len(package_data)} - "
                    f"Speed: {rps:.1f} records/sec - "
                    f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")
                
                offset += batch_size
                
                # Break if we got fewer records than expected (end of data)
                if batch_count < batch_size:
                    break
        
        # Write to JSON file
        print(f"Writing {len(package_data)} packages to {filename}...")
        with open(filename, 'w') as f:
            json.dump(package_data, f, indent=2)
        
        total_time = time.time() - start_time
        print(f"Completed processing {processed_count} records in {total_time:.1f} seconds")
        print(f"Found {len(package_data)} unique packages")
        print(f"Exported package CVE versions to {filename}")
        return True

    def validate_package_coverage(self, results):
        """
        Validate that the minimal version set for each package covers all its CVEs.
        
        Args:
            results: Dictionary output from build_minimal_hitting_sets_per_package
            
        Returns:
            Dictionary with validation statistics
        """
        validation_stats = {
            "total_packages": len(results),
            "packages_with_full_coverage": 0,
            "packages_with_partial_coverage": 0,
            "coverage_percentages": {},
            "problematic_packages": []
        }
        
        for package_name, package_data in results.items():
            minimal_versions = package_data.get("minimal_versions", [])
            vulnerabilities = package_data.get("vulnerabilities", {})
            
            if not vulnerabilities or not minimal_versions:
                validation_stats["problematic_packages"].append({
                    "package": package_name,
                    "issue": "Missing data",
                    "vuln_count": len(vulnerabilities),
                    "min_set_size": len(minimal_versions)
                })
                continue
            
            # Check coverage for each CVE
            total_cves = len(vulnerabilities)
            covered_cves = 0
            
            for cve_id, affected_versions in vulnerabilities.items():
                # A CVE is covered if any of its affected versions is in the minimal set
                if any(version in minimal_versions for version in affected_versions):
                    covered_cves += 1
            
            # Calculate coverage percentage
            coverage_pct = (covered_cves / total_cves) * 100 if total_cves > 0 else 0
            validation_stats["coverage_percentages"][package_name] = coverage_pct
            
            if coverage_pct == 100:
                validation_stats["packages_with_full_coverage"] += 1
            else:
                validation_stats["packages_with_partial_coverage"] += 1
                validation_stats["problematic_packages"].append({
                    "package": package_name,
                    "issue": f"Partial coverage: {coverage_pct:.1f}%",
                    "covered": f"{covered_cves}/{total_cves}"
                })
        
        # Overall coverage statistics
        all_percentages = list(validation_stats["coverage_percentages"].values())
        validation_stats["average_coverage"] = sum(all_percentages) / len(all_percentages) if all_percentages else 0
        validation_stats["min_coverage"] = min(all_percentages) if all_percentages else 0
        validation_stats["max_coverage"] = max(all_percentages) if all_percentages else 0
        
        return validation_stats
    
    def generate_final_report(self, results, validation_stats, output_file="final_report.json"):
        """
        Generate a final report with summary and details for all packages.
        
        Args:
            results: Dictionary output from build_minimal_hitting_sets_per_package
            validation_stats: Dictionary output from validate_package_coverage
            output_file: Filename for the output JSON
        """
        report = {
            "summary": {
                "total_packages": validation_stats["total_packages"],
                "total_cves": sum(len(pkg["vulnerabilities"]) for pkg in results.values()),
                "full_coverage_percent": (validation_stats["packages_with_full_coverage"] / 
                                        validation_stats["total_packages"] * 100) if validation_stats["total_packages"] > 0 else 0,
                "average_coverage": validation_stats["average_coverage"],
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "average_minimal_set_size": sum(pkg.get("min_cover_size", 0) for pkg in results.values()) / 
                                            len(results) if results else 0
            },
            "packages": {}
        }
        
        # Include detailed information for each package
        for package_name, package_data in results.items():
            vuln_count = len(package_data.get("vulnerabilities", {}))
            min_set = package_data.get("minimal_versions", [])
            coverage = validation_stats["coverage_percentages"].get(package_name, 0)
            
            report["packages"][package_name] = {
                "ecosystem": package_data.get("ecosystem", "unknown"),
                "vulnerability_count": vuln_count,
                "minimal_version_set": min_set,
                "minimal_set_size": len(min_set),
                "coverage_percentage": coverage
            }
        
        # Save the report
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"Final report saved to {output_file}")
        
        # Also create a simplified CSV version for easy analysis
        csv_file = output_file.replace(".json", ".csv")
        with open(csv_file, 'w') as f:
            f.write("Package,Ecosystem,Vulnerability Count,Minimal Set Size,Coverage Percentage\n")
            for package, data in report["packages"].items():
                f.write(f"{package},{data['ecosystem']},{data['vulnerability_count']},"
                    f"{data['minimal_set_size']},{data['coverage_percentage']}\n")
        
        print(f"CSV summary saved to {csv_file}")
        
        return report

def find_minimum_hitting_set(cve_version_lists, version_recency=None):
    """
    Returns a minimal set of versions that covers all CVEs in cve_version_lists.
    Breaks ties by choosing the set with maximum sum of recency scores.
    
    Args:
        cve_version_lists: List of lists where each sublist contains versions affected by a specific CVE
        version_recency: Dictionary mapping versions to their recency scores (higher = more recent)
                         If None, will not prioritize by recency
    
    Returns:
        List of versions forming the minimum hitting set
    """
    from ortools.sat.python import cp_model
    import time
    import logging

    # sets up logging

    logger = logging.getLogger(__name__)

    start_time = time.time()

    # Input validation and cleanup
    if not cve_version_lists:
        logger.warning("Empty input to find_minimum_hitting_set")
        return []

    if not isinstance(cve_version_lists, list):
        logger.error(f"Input must be a list, got {type(cve_version_lists)}")
        return []

    # Filter out empty/invalid version lists
    non_empty_lists = []
    for lst in cve_version_lists:
        if lst and isinstance(lst, (list, tuple)):
            # Keep only non-empty lists/tuples
            non_empty_lists.append(list(lst))
        
    if not non_empty_lists:
        logger.warning("No valid CVE version lists to process (all empty)")
        return []
    
    # Get all unique versions across all CVEs
    unique_versions = set(v for sublist in non_empty_lists for v in sublist)
    if not unique_versions:
        logger.warning("No versions found in the input lists")
        return []

    # If version_recency is not provided, create a default one with all zeros
    if version_recency is None:
        version_recency = {}

    # Create a dictionary with recency scores, defaulting to 0 if not specified
    rec = {v: version_recency.get(v, 0) for v in unique_versions}

    # Sort versions by recency (highest first) to give solver a hint about preferred order
    # This helps when multiple optimal solutions exist
    all_versions = sorted(unique_versions, key=lambda v: rec.get(v, 0), reverse=True)

    logger.info(f"Processing {len(non_empty_lists)} CVEs with {len(all_versions)} unique versions")
    
    logger.info("Phase 1: Finding minimum hitting set cardinality")
    phase1_start = time.time()

    try: 
        # Set up the CP-SAT model
        model = cp_model.CpModel()
        
        # Create boolean variables for each version
        x = {}
        for v in all_versions:
            x[v] = model.NewBoolVar(v)  # Boolean variable indicating if version v is in the solution

        # Add constraints to ensure each CVE is covered by at least one version
        for i, cve_list in enumerate(non_empty_lists):
            model.Add(sum(x[v] for v in cve_list) >= 1)
    
        # Create a variable for the total number of versions in the solution
        cardinality = model.NewIntVar(0, len(all_versions), "cardinality")
        model.Add(cardinality == sum(x[v] for v in all_versions))

        # Minimize the number of versions
        model.Minimize(cardinality)
    
        # set up solver with timeout to avoid hanging on very large problems
        solver = cp_model.CpSolver()
        solver.parameters.max_time_in_seconds = 300  # 5 min timeout

        # Solve phase 1
        status = solver.Solve(model)

        # Handle solver status
        if status == cp_model.OPTIMAL:
            logger.info("Found optimal solution in phase 1")
        elif status == cp_model.FEASIBLE:
            logger.info("Found feasible (but possibly not optimal) solution in phase 1")
        else:
            logger.warning(f"Phase 1 solver status: {status} - no solution found")
            return []
        
        # Get the minimum cardinality value
        min_cardinality = int(solver.ObjectiveValue())
        logger.info(f"Minimum hitting set cardinality: {min_cardinality}")

         # If all recency scores are the same (or zero), we can skip phase 2
        if len(set(rec.values())) <= 1:
            logger.info("All versions have same recency score - skipping phase 2")
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)

            phase1_time = time.time() - phase1_start
            logger.info(f"Phase 1 completed in {phase1_time:.2f} seconds")
            return chosen_versions
        
        # Phase 2: Among all minimum hitting sets, select the one with maximum recency
        logger.info("Phase 2: Maximizing recency while maintaining minimum cardinality")
        phase2_start = time.time()

        model2 = cp_model.CpModel()
        x2 = {}
        for v in all_versions:
            x2[v] = model2.NewBoolVar(f'select_{v}')
        
        # Constraint: Each CVE must be covered by at least one selected version
        for i, cve_list in enumerate(non_empty_lists):
            model2.Add(sum(x2[v] for v in cve_list) >= 1)

        # Constraint: Total number of selected versions must equal minimum cardinality
        model2.Add(sum(x2.values()) == min_cardinality)

        # Objective: Maximize recency scores
        # Scale recency values by a large factor to avoid precision loss
        scale_factor = 1000000  # Use 1 million to handle decimal recency scores

        objective_terms = []
        for v in all_versions:
            recency_score = rec.get(v, 0)
            # Convert to integer coefficient (CP-SAT requires integer coefficients)
            recency_int = int(recency_score * scale_factor)
            if recency_int != 0:  # Only add non-zero terms
                objective_terms.append(recency_int * x2[v])
        
        if objective_terms:
            model2.Maximize(sum(objective_terms))
        else:
            logger.warning("No recency scores to optimize, using phase 1 result")
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)
            return chosen_versions
        
        # Solve phase 2
        solver2 = cp_model.CpSolver()
        solver2.parameters.max_time_in_seconds = 300.0  # 5 minute timeout
        status2 = solver2.Solve(model2)
        
        phase2_end = time.time()
        
        # Handle solver status
        if status2 == cp_model.OPTIMAL:
            logger.info("Found optimal solution in phase 2")
        elif status2 == cp_model.FEASIBLE:
            logger.warning("Found feasible (but possibly not optimal) solution in phase 2")
        else:
            logger.error(f"Phase 2 failed with status: {solver2.StatusName(status2)}, using phase 1 result")
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)
            
            total_time = time.time() - start_time
            logger.info(f"Total solving time: {total_time:.2f} seconds")
            return chosen_versions
        
        # Extract the solution from phase 2
        chosen_versions = []
        for v in all_versions:
            if solver2.BooleanValue(x2[v]):
                chosen_versions.append(v)

        logger.info(f"Phase 2 completed in {phase2_end - phase2_start:.2f} seconds")
        
        total_time = time.time() - start_time
        logger.info(f"Total solving time: {total_time:.2f} seconds")
        
        # Verify the solution is valid
        covered_cves = 0
        for cve_list in non_empty_lists:
            if any(v in chosen_versions for v in cve_list):
                covered_cves += 1
        
        coverage_pct = (covered_cves / len(non_empty_lists)) * 100
        logger.info(f"Solution covers {covered_cves}/{len(non_empty_lists)} CVEs ({coverage_pct:.1f}%)")
        
        return chosen_versions
    
    except Exception as e:
        logger.error(f"Error solving minimum hitting set: {e}")
        return []

def main():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    
    try:
        if mapper.connect():
            print("Successfully connected to Neo4j database.")
            
            total_vulns = mapper.get_vulnerability_count()
            print(f"Found {total_vulns} vulnerability relationships in the database for repo 'OSV'.")
            
            if total_vulns > 100000:
                print("Large dataset detected. Using memory-efficient streaming export...")
                mapper.export_to_json_streaming()
            else:
                print("Using standard batch processing...")
                mapper.export_to_json()

            # uses the exported file to build minimal hitting sets
            print("\nBuilding minimal hitting sets from exported data...")
            results = mapper.build_minimal_hitting_sets_per_package()
            
            if results:
                print(f"Successfully processed minimal hitting sets for {len(results)} packages")
                # statistics about the results
                total_versions = sum(pkg["min_cover_size"] for pkg in results.values())
                print(f"Total number of versions in all minimal sets: {total_versions}")
                print(f"Average minimal set size: {total_versions/len(results) if results else 0:.2f}")
            else:
                print("No results returned from minimal hitting sets calculation")

        else:
            print("Failed to connect to Neo4j database.")
    
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        mapper.close()

if __name__ == "__main__":
    main()
