"""
Vulnerability Scanner using LLM for code analysis
Implements the vulnerability detection prompts
"""

import json
import logging
import httpx
from typing import List, Dict, Any, Optional, Literal
from pydantic import BaseModel, Field
import os
import yaml

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models for vulnerability analysis
class Lead(BaseModel):
    headline: str = Field(description="a short description of the lead")
    analysis: str = Field(description="in-depth explanation and investigation of the lead. Several sentences at least. Do not include security recommendations: the goal here is to get security researchers started with development of a POC exploit.")
    cwe: str = Field(description="root cause of the vulnerability, as the most specific CWE ID in the list below that applies to the vulnerability. Only include an ID (e.g. CWE-999), not the description of the CWE.")
    function_names: List[str] = Field(description="a list of up to 3 function names where the vulnerability is present. The list may be empty if the vulnerability doesn't map cleanly to specific functions, e.g. some race conditions.")
    filenames: List[str] = Field(description="a list of up to 3 filenames where the vulnerability is present. The list may be empty if the vulnerability doesn't map cleanly to specific files. Filenames must be listed as they are written below, i.e. with their full path relative to the root of the repository.")
    classification: Literal["very promising", "slightly promising", "not promising"]

class LeadList(BaseModel):
    leads: List[Lead]

class ScoreResponse(BaseModel):
    reasoning: str = Field(description="Your reasoning behind the score.")
    score: Literal[0, 1] = Field(description="The score for the vulnerability submission. Must be 0 or 1.")
    corresponds_to: Optional[str] = Field(description="The ID of the vulnerability that the submission corresponds to, if and only if the score is 1. Do not include if score is 0.", default=None)

class VulnerabilityScanner:
    def __init__(self, ollama_url: str = "http://ollama:11434", model_name: str = "llama3.2:1b"):
        self.ollama_url = ollama_url
        self.model_name = model_name
        timeout = httpx.Timeout(900.0, connect=30.0, read=900.0, write=300.0)
        self.client = httpx.AsyncClient(timeout=timeout)  # 15 minute overall timeout with generous read window

        # Common Web Application Security Project (OWASP) CWE list
        self.cwe_list = """
CWE-20: Improper Input Validation
CWE-22: Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')
CWE-78: Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')
CWE-79: Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')
CWE-89: Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')
CWE-94: Improper Control of Generation of Code ('Code Injection')
CWE-119: Improper Restriction of Operations within the Bounds of a Memory Buffer
CWE-125: Out-of-bounds Read
CWE-190: Integer Overflow or Wraparound
CWE-200: Exposure of Sensitive Information to an Unauthorized Actor
CWE-209: Information Exposure Through Error Messages
CWE-269: Improper Privilege Management
CWE-276: Incorrect Default Permissions
CWE-287: Improper Authentication
CWE-295: Improper Certificate Validation
CWE-311: Missing Encryption of Sensitive Data
CWE-319: Cleartext Transmission of Sensitive Information
CWE-321: Use of Hard-coded Cryptographic Key
CWE-352: Cross-Site Request Forgery (CSRF)
CWE-362: Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')
CWE-400: Uncontrolled Resource Consumption
CWE-434: Unrestricted Upload of File with Dangerous Type
CWE-476: NULL Pointer Dereference
CWE-502: Deserialization of Untrusted Data
CWE-522: Insufficiently Protected Credentials
CWE-611: Improper Restriction of XML External Entity Reference
CWE-732: Incorrect Permission Assignment for Critical Resource
CWE-787: Out-of-bounds Write
CWE-798: Use of Hard-coded Credentials
CWE-862: Missing Authorization
CWE-918: Server-Side Request Forgery (SSRF)
CWE-943: Improper Neutralization of Special Elements in Data Query Logic
"""

    async def analyze_code_chunk(self, code_chunk: str, file_path: str = "") -> Optional[LeadList]:
        """
        Analyze a code chunk for security vulnerabilities using LLM
        
        Args:
            code_chunk: The code to analyze
            file_path: Optional file path for context
            
        Returns:
            LeadList object with identified vulnerabilities or None if analysis fails
        """
        
        prompt = f"""Identify all the security vulnerabilities in the codebase below.
Your reply must be a valid YAML object equivalent to type LeadList, according to the following
Pydantic definitions:

```python
class Lead(BaseModel):
    headline: str = Field(description="a short description of the lead")
    analysis: str = Field(description="in-depth explanation and investigation of the lead. Several sentences at least. Do not include security recommendations: the goal here is to get security researchers started with development of a POC exploit.")
    cwe: str = Field(description="root cause of the vulnerability, as the most specific CWE ID in the list below that applies to the vulnerability. Only include an ID (e.g. CWE-999), not the description of the CWE.")
    function_names: list[str] = Field(description="a list of up to 3 function names where the vulnerability is present. The list may be empty if the vulnerability doesn't map cleanly to specific functions, e.g. some race conditions.")
    filenames: list[str] = Field(description="a list of up to 3 filenames where the vulnerability is present. The list may be empty if the vulnerability doesn't map cleanly to specific files. Filenames must be listed as they are written below, i.e. with their full path relative to the root of the repository.")
    classification: Literal["very promising", "slightly promising", "not promising"]

class LeadList(BaseModel):
    leads: list[Lead]
```

Example YAML output:
```yaml
leads:
- headline: SQL Injection in User Authentication
  analysis: |
    The login function directly concatenates user input into SQL queries without proper sanitization.
    An attacker could inject malicious SQL code through the username parameter to bypass authentication
    or extract sensitive data from the database. The vulnerability occurs because the query construction
    uses string formatting instead of parameterized queries.
  cwe: CWE-89
  function_names:
    - login_user
    - authenticate
  filenames:
    - src/auth/login.py
  classification: very promising
```

Start your answer with:
```yaml

Below is the CWE list, for your reference. Do NOT copy that list in your response.
<CWE list reference>
{self.cwe_list}
</CWE list reference>

<><><>codebase<><><>
File: {file_path}

{code_chunk}
<><><>"""

        try:
            response = await self._call_ollama(prompt)
            if not response:
                return None
                
            # Extract YAML from response
            yaml_content = self._extract_yaml(response)
            if not yaml_content:
                logger.warning("No YAML content found in LLM response")
                return None
                
            # Parse YAML to LeadList
            try:
                parsed_data = yaml.safe_load(yaml_content)
                return LeadList(**parsed_data)
            except (yaml.YAMLError, Exception) as e:
                logger.error(f"Failed to parse YAML response: {e}")
                logger.debug(f"Raw YAML content: {yaml_content}")
                return None
                
        except Exception as e:
            logger.error(f"Error analyzing code chunk: {e}")
            return None

    async def score_vulnerability(self, vuln_submission: Dict[str, Any], real_vulnerabilities: List[Dict[str, Any]]) -> Optional[ScoreResponse]:
        """
        Score a vulnerability submission against real vulnerabilities
        
        Args:
            vuln_submission: The vulnerability submission to score
            real_vulnerabilities: List of real vulnerabilities to compare against
            
        Returns:
            ScoreResponse object with score and reasoning
        """
        
        submission_text = f"""headline: {vuln_submission.get('headline', '')}
analysis: {vuln_submission.get('analysis', '')}
cwe: {vuln_submission.get('cwe', '')}
function_names: {vuln_submission.get('function_names', [])}
filenames: {vuln_submission.get('filenames', [])}"""

        vulns_text = "\n".join([
            f'{{"id": "{vuln.get("id", "")}", "details": "{vuln.get("details", "")}"}}' 
            for vuln in real_vulnerabilities
        ])

        prompt = f"""An AI vulnerability detection tool has analyzed a project and come up with a possible lead, included below.

Your task is to score this lead based uniquely on whether it corresponds to one of the actual vulnerabilities displayed below.

The score should be 1 if the lead corresponds to one of the actual vulnerabilities, and 0 if it doesn't correspond to any of them, or if it kind of corresponds to one, but is significantly off or imprecise (for instance if it's overly broad, or doesn't mention a realistic attack vector).

If and only if the score is 1, the corresponding real vulnerability ID must be returned. It is extracted from the "id" field of the vulnerability.

Your reply must be a valid YAML object equivalent to type ScoreResponse, according to the following Pydantic definition:

```python
class ScoreResponse(BaseModel):
    reasoning: str = Field(description="Your reasoning behind the score.")
    score: Literal[0, 1] = Field(description="The score for the vulnerability submission. Must be 0 or 1.")
    corresponds_to: str | None = Field(description="The ID of the vulnerability that the submission corresponds to, if and only if the score is 1. Do not include if score is 0.", default=None)
```

<vulnerability_submission>
{submission_text}
</vulnerability_submission>

<vulnerabilities>
{vulns_text}
</vulnerabilities>

Response:
```yaml"""

        try:
            response = await self._call_ollama(prompt)
            if not response:
                return None
                
            yaml_content = self._extract_yaml(response)
            if not yaml_content:
                logger.warning("No YAML content found in scoring response")
                return None
                
            try:
                parsed_data = yaml.safe_load(yaml_content)
                return ScoreResponse(**parsed_data)
            except (yaml.YAMLError, Exception) as e:
                logger.error(f"Failed to parse scoring YAML response: {e}")
                return None
                
        except Exception as e:
            logger.error(f"Error scoring vulnerability: {e}")
            return None

    async def _call_ollama(self, prompt: str) -> Optional[str]:
        """Call Ollama API with the given prompt"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 4000
                }
            }
            
            response = await self.client.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                logger.error(f"Ollama API error: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"Error calling Ollama API: {e}")
            return None

    def _extract_yaml(self, text: str) -> Optional[str]:
        """Extract YAML content from LLM response"""
        # Look for YAML code blocks
        import re
        
        # Try to find YAML between ```yaml and ```
        yaml_pattern = r'```yaml\s*\n(.*?)\n```'
        match = re.search(yaml_pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()
        
        # Try to find YAML between ``` and ```
        yaml_pattern = r'```\s*\n(.*?)\n```'
        match = re.search(yaml_pattern, text, re.DOTALL)
        if match:
            content = match.group(1).strip()
            # Check if it looks like YAML (contains leads:)
            if 'leads:' in content or 'reasoning:' in content:
                return content
        
        # If no code blocks, try to extract from the response directly
        lines = text.split('\n')
        yaml_start = -1
        for i, line in enumerate(lines):
            if line.strip().startswith('leads:') or line.strip().startswith('reasoning:'):
                yaml_start = i
                break
        
        if yaml_start >= 0:
            return '\n'.join(lines[yaml_start:]).strip()
        
        return None

    async def close(self):
        """Close the HTTP client"""
        await self.client.aclose()

# Singleton instance
scanner = VulnerabilityScanner()