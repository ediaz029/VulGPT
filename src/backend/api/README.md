## Backend Tasks Overview

This backend workspace contains the Python utilities that power the vulnerability benchmarking pipeline:

- `filter_minimal_sets.py` enriches OSV data and produces the curated package list.
- `checkout_repos.py` clones the repositories and writes a checkout manifest.
- `run_vulnerability_scanner.py` traverses the checked-out worktrees, chunks code, and calls a local Ollama model to surface vulnerability leads. When `--score-leads` is supplied it also evaluates each lead against OSV ground truth to produce precision/recall metrics.

The sections below focus on running the scanner so you can reproduce our dry-run and full-scan flows.

## Prerequisites

- Python dependencies can be installed either with [uv](https://github.com/astral-sh/uv)
  or `pip`. Choose one of the workflows below:

	```bash
	# Using uv (recommended)
	uv sync

	# Or with pip
	python3 -m venv .venv
	source .venv/bin/activate
	pip install -r src/backend/api/requirements.txt
	```

- Ollama must be installed locally with the target model pulled (e.g., `ollama pull mistral:7b-instruct-q4_0`)
  and the daemon running on `http://localhost:11434`.
- The checkout manifest generated by `checkout_repos.py` must be available (e.g.,
  `data/checkout_manifest.json` in the repo root).

## Dry run: validate wiring without LLM usage

From the repository root, execute the scanner with `--dry-run` to confirm manifest loading, traversal, and chunking:

```bash
PYTHONPATH=src uv run --project src/backend/api python src/backend/tasks/run_vulnerability_scanner.py \
	--manifest data/checkout_manifest_sample.json \
	--output data/scanner_dry_run.json \
	--model mistral:7b-instruct-q4_0 \
	--include-ext .py .js \
	--dry-run \
	--log-level INFO
```

The resulting JSON captures the resolved options plus chunk metadata (no LLM calls are issued when `dry_run` is true).

## Full scan: collect vulnerability leads

Remove `--dry-run` to let the scanner submit each chunk to Ollama and gather YAML findings:

```bash
PYTHONPATH=src uv run --project src/backend/api python src/backend/tasks/run_vulnerability_scanner.py \
	--manifest data/checkout_manifest_sample.json \
	--output data/scanner_findings.json \
	--model mistral:7b-instruct-q4_0 \
	--include-ext .py .js \
	--log-level INFO
```

Additional flags you may find useful:

- `--max-files`, `--max-chunks`: limit work per package to control runtime.
- `--exclude-dirs`: extend the default ignore list.
- `--concurrency`: balance throughput vs. Ollama load (default `2`).
- `--max-packages`: cap the number of manifest entries processed.
- `--chunk-size`, `--adaptive-chunking`: tune chunk lengths when repositories contain very large files.
- `--max-file-bytes`: skip oversized assets during traversal.

## Post-scan scoring and metrics

Add the `--score-leads` switch to run the new ADK-style scoring pipeline immediately
after the scan:

```bash
SCORING_ENDPOINT=https://your-scorer.example.com \
SCORING_API_KEY=example-key \
PYTHONPATH=src uv run --project src/backend/api python src/backend/tasks/run_vulnerability_scanner.py \
	--manifest data/checkout_manifest.json \
	--output data/scanner_scored.json \
	--model mistral:7b-instruct-q4_0 \
	--score-leads \
	--log-level INFO
```

- When `SCORING_ENDPOINT` is omitted the tool falls back to the local Ollama-based
  scorer (`VulnerabilityScanner.score_vulnerability`).
- Set `--no-local-scoring-fallback` if the external service should be treated as
  authoritative.
- OSV lookups power the ground-truth comparison. Adjust `--scoring-timeout` and
  `--scoring-max-osv-retries` if you encounter transient API failures.
- The resulting JSON now includes a `scoring` array with per-lead judgments and a
  `metrics` object summarising per-package and aggregate precision/recall counts.

### Running the ADK scoring service locally

The scanner expects a scoring service that implements the ADK workflow. This repository
ships a ready-made FastAPI wrapper around a Gemini-powered ADK agent:

1. Ensure dependencies (including `google-adk`) are installed:

	```bash
	python3 -m venv .venv
	source .venv/bin/activate
	pip install -r src/backend/api/requirements.txt
	```

2. Provide your Gemini API key by copying the example environment file:

	```bash
	cp src/backend/adk_scorer/.env.example src/backend/adk_scorer/.env
	# edit the file and replace the placeholder with your real key
	```

3. Launch the scoring service (defaults to port 9000):

	```bash
	PYTHONPATH=src uvicorn backend.adk_scorer.main:app --host 0.0.0.0 --port 9000
	```

4. Point the scanner at the local endpoint:

	```bash
	SCORING_ENDPOINT=http://localhost:9000/score PYTHONPATH=src \
	uv run --project src/backend/api python src/backend/tasks/run_vulnerability_scanner.py \
	    --manifest data/checkout_manifest.json \
	    --output data/scanner_scored.json \
	    --model mistral:7b-instruct-q4_0 \
	    --score-leads
	```

The service also exposes a simple health check at `/health`. You can interact with the
agent directly using ADKâ€™s CLI (`adk run src/backend/adk_scorer`) if you want to inspect
its reasoning outside the HTTP wrapper.

## Output interpretation

- Dry runs produce an empty `results` array and a `stats` object describing chunk counts.
- Full runs populate `results` with one entry per discovered lead, including the originating file, chunk index, model response, and timestamps. These JSON artifacts feed downstream scoring or manual triage.
- When scoring is enabled, review the `scoring` list for lead-level scores and the
	`metrics` block for aggregated precision/recall-style measurements inspired by the
	eyeballvul paper.

## Troubleshooting

- **Missing dependencies**: run `uv sync` or `pip install -r src/backend/api/requirements.txt` again to ensure `httpx` and `PyYAML` are installed.
- **Scoring timeouts**: increase `--scoring-timeout` or temporarily disable scoring to confirm the base scan still succeeds.
	- If the ADK service logs `GOOGLE_API_KEY` errors, confirm the `.env` file is present in `src/backend/adk_scorer/` or export the variable in your shell.
